import streamlit as st
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import os
import json
import pickle
import io
import time
from openai import OpenAI  # ç”¨äºè°ƒç”¨ DeepSeek

# ==============================================================================
# ğŸ› ï¸ 1. é…ç½®ä¸æ¨¡å‹è·¯å¾„
# ==============================================================================
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 1081
WEIGHTS_PATH = 'data.pkl'
SPECIES_NAME_JSON = 'plantnet300K_species_id_2_name.json'
CLASS_TO_ID_JSON = 'class_idx_to_species_id.json'

# ==============================================================================
# ğŸ§  2. æ ¸å¿ƒé€»è¾‘ï¼šåŠ è½½æœ¬åœ° ResNet æ¨¡å‹
# ==============================================================================
@st.cache_resource
def load_resources():
    """åŠ è½½æœ¬åœ° PyTorch æ¨¡å‹å’Œç±»åˆ«æ˜ å°„"""
    if not os.path.exists(SPECIES_NAME_JSON) or not os.path.exists(CLASS_TO_ID_JSON):
        st.error("âŒ ç¼ºå°‘ JSON é…ç½®æ–‡ä»¶ã€‚")
        return None, None

    with open(CLASS_TO_ID_JSON, 'r', encoding='utf-8') as f:
        class_to_id = json.load(f)
    with open(SPECIES_NAME_JSON, 'r', encoding='utf-8') as f:
        species_id_to_name = json.load(f)
    
    class_names = []
    for i in range(NUM_CLASSES):
        species_id = str(class_to_id[str(i)])
        class_names.append(species_id_to_name.get(species_id, f"Unknown {species_id}"))

    if not os.path.exists(WEIGHTS_PATH):
        st.error(f"âŒ ç¼ºå°‘æƒé‡æ–‡ä»¶: {WEIGHTS_PATH}")
        return None, None

    base_dir = os.path.dirname(os.path.abspath(WEIGHTS_PATH))
    data_dir = os.path.join(base_dir, 'data')

    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)

    # æ·±åº¦å…¼å®¹ Unpickler (è§£å†³æ—§ç‰ˆæ•°æ®æ ¼å¼)
    class CustomUnpickler(pickle.Unpickler):
        def find_class(self, module, name):
            if module == 'torch.storage' and name == '_load_from_bytes':
                return lambda b: torch.load(io.BytesIO(b), map_location='cpu', weights_only=False)
            return super().find_class(module, name)

        def persistent_load(self, saved_id):
            if isinstance(saved_id, tuple) and saved_id[0] == 'storage':
                typename, key, _, numel = saved_id[1], saved_id[2], saved_id[3], saved_id[4]
                typename_str = typename.__name__ if isinstance(typename, type) else str(typename)
                storage_cls = torch.FloatStorage
                if 'LongStorage' in typename_str: storage_cls = torch.LongStorage
                elif 'IntStorage' in typename_str: storage_cls = torch.IntStorage
                
                data_file_path = os.path.join(data_dir, str(key))
                # å¦‚æœ data æ–‡ä»¶å¤¹ç¼ºå¤±ï¼Œè®©å®ƒæŠ¥é”™ä»¥ä¾¿ç”¨æˆ·å‘ç°
                return storage_cls.from_file(data_file_path, shared=False, size=numel)
            return saved_id

    try:
        with open(WEIGHTS_PATH, 'rb') as f:
            checkpoint = CustomUnpickler(f).load()
        state_dict = checkpoint['model'] if isinstance(checkpoint, dict) and 'model' in checkpoint else checkpoint
        model.load_state_dict(state_dict)
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

    model = model.to(device)
    model.eval()
    return model, class_names

def predict_local(image, model, class_names):
    """æœ¬åœ°æ¨¡å‹æ¨ç†ï¼Œåªè¿”å›æ‹‰ä¸åå’Œç½®ä¿¡åº¦"""
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    input_tensor = preprocess(image).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(input_tensor)
        probs = torch.nn.functional.softmax(outputs, dim=1)[0]
        top_p, top_idx = probs.topk(1)
    return class_names[top_idx.item()], top_p.item() * 100

# ==============================================================================
# ğŸ¤– 3. äº‘ç«¯é€»è¾‘ï¼šè°ƒç”¨ DeepSeek API
# ==============================================================================
def ask_deepseek(api_key, latin_name, location, season):
    """è°ƒç”¨ DeepSeek è·å–è¯¦ç»†ç§‘æ™®"""
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    # æ„å»º Prompt
    system_prompt = "ä½ æ˜¯ä¸€ä½åšå­¦çš„æ¤ç‰©å­¦å®¶å’Œè‡ªç„¶æ•™è‚²å®¶ã€‚è¯·ç”¨ç”ŸåŠ¨ã€å‡†ç¡®çš„ä¸­æ–‡ä»‹ç»æ¤ç‰©ã€‚"
    user_prompt = f"""
    ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ æ¤ç‰©ç…§ç‰‡ï¼Œç»è¯†åˆ«å…¶æ‹‰ä¸å­¦åä¸ºï¼š"{latin_name}"ã€‚
    ç”¨æˆ·å‘ç°å®ƒçš„ç¯å¢ƒä¿¡æ¯ï¼šåœ°ç‚¹="{location}"ï¼Œå­£èŠ‚="{season}"ã€‚

    è¯·ç”Ÿæˆä¸€ä»½åŒ…å«ä»¥ä¸‹å†…å®¹çš„ç§‘æ™®æŠ¥å‘Šï¼ˆä½¿ç”¨Markdownæ ¼å¼ï¼‰ï¼š
    1. **ä¸­æ–‡æ­£å**ï¼šç»™å‡ºæœ€é€šç”¨çš„ä¸­æ–‡åç§°ã€‚
    2. **æ¤ç‰©ç®€ä»‹**ï¼šç®€è¦ä»‹ç»å®ƒçš„ç§‘å±ã€åŸäº§åœ°å’Œä¸»è¦å½¢æ€ç‰¹å¾ã€‚
    3. **ç”Ÿé•¿ä¹ æ€§**ï¼šå®ƒå–œæ¬¢ä»€ä¹ˆæ ·çš„åœŸå£¤ã€å…‰ç…§å’Œæ°´åˆ†ï¼Ÿ
    4. **ç¯å¢ƒäº’åŠ¨**ï¼šç»“åˆç”¨æˆ·æä¾›çš„åœ°ç‚¹ï¼ˆ{location}ï¼‰å’Œå­£èŠ‚ï¼ˆ{season}ï¼‰ï¼Œåˆ†æä¸ºä»€ä¹ˆå®ƒä¼šå‡ºç°åœ¨è¿™é‡Œï¼Ÿæœ‰ä»€ä¹ˆè§‚å¯Ÿå»ºè®®ï¼Ÿ
    5. **è¶£å‘³å†·çŸ¥è¯†**ï¼šå…³äºè¿™ç§æ¤ç‰©çš„ä¸€ä¸ªæœ‰è¶£äº‹å®æˆ–è¯ç”¨/ç»æµä»·å€¼ã€‚

    è¯·ä¿æŒè¯­æ°”äº²åˆ‡ã€ä¸“ä¸šï¼Œå­—æ•°æ§åˆ¶åœ¨400å­—ä»¥å†…ã€‚
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=1.3, # ç¨å¾®é«˜ä¸€ç‚¹ï¼Œè®©å›ç­”æ›´ç”ŸåŠ¨
            stream=True      # å¼€å¯æµå¼è¾“å‡º
        )
        return response
    except Exception as e:
        return f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}"

# ==============================================================================
# ğŸ¨ 4. å‰ç«¯ç•Œé¢
# ==============================================================================
st.set_page_config(page_title="æ¤ç‰©è¯†åˆ« Pro (DeepSeekç‰ˆ)", page_icon="ğŸŒ¿", layout="wide")

st.markdown("""
<style>
    .stButton>button { width: 100%; border-radius: 8px; font-weight: bold; }
    .report-box { border: 2px solid #f0f2f6; padding: 20px; border-radius: 10px; background-color: #ffffff; }
</style>
""", unsafe_allow_html=True)

# --- ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®")
    st.markdown("æœ¬ç³»ç»Ÿé‡‡ç”¨ **ç«¯äº‘ç»“åˆ** æ¶æ„ï¼š")
    st.info("ğŸ–¥ï¸ **æœ¬åœ° ResNet**ï¼šæ¯«ç§’çº§è¯†åˆ«æ¤ç‰©èº«ä»½")
    st.info("â˜ï¸ **DeepSeek AI**ï¼šç”Ÿæˆæ·±åº¦ç§‘æ™®ä»‹ç»")
    
    api_key = st.text_input("ğŸ”‘ è¾“å…¥ DeepSeek API Key", type="password", placeholder="sk-...")
    if not api_key:
        st.warning("âš ï¸ è¯·å…ˆè¾“å…¥ API Key æ‰èƒ½è·å–è¯¦ç»†ä»‹ç»")
        st.markdown("[ğŸ‘‰ ç‚¹å‡»ç”³è¯· DeepSeek Key](https://platform.deepseek.com/)")

# --- ä¸»ç•Œé¢ ---
st.title("ğŸŒ¿ AI æ¤ç‰©ç™¾ç§‘å…¨ä¹¦")
st.caption("Powered by PyTorch & DeepSeek-V3")

# åŠ è½½æœ¬åœ°æ¨¡å‹
with st.spinner('æ­£åœ¨åŠ è½½æœ¬åœ°è§†è§‰æ¨¡å‹...'):
    model, class_names = load_resources()

if not model:
    st.stop()

col1, col2 = st.columns([1, 1.5])

with col1:
    st.subheader("1. æ‹æ‘„/ä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=["jpg", "png", "jpeg"])
    
    st.subheader("2. ç¯å¢ƒä¿¡æ¯ (AIå°†ç»“åˆæ­¤ä¿¡æ¯)")
    location = st.text_input("ğŸ“ å‘ç°åœ°ç‚¹", value="")
    season = st.selectbox("ğŸ—“ï¸ å½“å‰å­£èŠ‚", ["æ˜¥å­£", "å¤å­£", "ç§‹å­£", "å†¬å­£"])
    
    identify_btn = st.button("ğŸš€ å¼€å§‹è¯†åˆ« & å’¨è¯¢ AI", type="primary")

with col2:
    if uploaded_file and identify_btn:
        # 1. å›¾ç‰‡æ˜¾ç¤º
        image = Image.open(uploaded_file).convert('RGB')
        st.image(image, caption="å¾…è¯†åˆ«å›¾åƒ", use_container_width=True)
        
        # 2. æœ¬åœ°æ¨ç† (æå¿«)
        start_time = time.time()
        latin_name, confidence = predict_local(image, model, class_names)
        local_time = time.time() - start_time
        
        st.success(f"è§†è§‰è¯†åˆ«å®Œæˆï¼(è€—æ—¶ {local_time:.3f}s)")
        
        # æ˜¾ç¤ºåˆæ­¥ç»“æœ
        c1, c2 = st.columns(2)
        c1.metric("è¯†åˆ«å­¦å", latin_name)
        c2.metric("è§†è§‰ç½®ä¿¡åº¦", f"{confidence:.1f}%")
        
        st.markdown("---")
        st.subheader("ğŸ¤– DeepSeek ç§‘æ™®æŠ¥å‘Š")

        # 3. è°ƒç”¨ DeepSeek (å¦‚æœå¡«äº† Key)
        if api_key:
            # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
            report_placeholder = st.empty()
            full_response = ""
            
            # è°ƒç”¨æµå¼ API
            stream = ask_deepseek(api_key, latin_name, location, season)
            
            if isinstance(stream, str): # å¦‚æœè¿”å›çš„æ˜¯é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
                st.error(stream)
            else:
                # å®æ—¶æ‰“å°å­—ç¬¦
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        report_placeholder.markdown(full_response + "â–Œ") # åŠ ä¸ªå…‰æ ‡ç‰¹æ•ˆ
                
                report_placeholder.markdown(full_response) # æœ€åæ˜¾ç¤ºå®Œæ•´å†…å®¹
        else:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ° API Keyï¼Œæ— æ³•ç”Ÿæˆä¸­æ–‡ä»‹ç»ã€‚åªèƒ½æ˜¾ç¤ºæ‹‰ä¸å­¦åã€‚")
            st.markdown(f"**Google ç¿»è¯‘é“¾æ¥ï¼š** [ç‚¹å‡»ç¿»è¯‘ {latin_name}](https://translate.google.com/?sl=la&tl=zh-CN&text={latin_name}&op=translate)")

    elif not uploaded_file:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ å›¾ç‰‡ã€‚")