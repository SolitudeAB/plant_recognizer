import json
import os
import time
import random
import requests
import re
from bs4 import BeautifulSoup
from tqdm import tqdm
import urllib3
from deep_translator import GoogleTranslator

# ç¦ç”¨å®‰å…¨è­¦å‘Šï¼Œå¹²å°±å®Œäº†
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================= é…ç½® =================
SOURCE_JSON = 'plantnet300K_species_id_2_name.json'
TARGET_JSON = 'plant_dictionary_zh.json'

# å¼ºåŠ›ä¼ªè£…å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': 'https://baike.baidu.com/',
    'Accept-Language': 'zh-CN,zh;q=0.9'
}

# ç¿»è¯‘å™¨å¤‡ç”¨
translator = GoogleTranslator(source='auto', target='zh-CN')

def safe_translate(text):
    """è°·æ­Œç¿»è¯‘å…œåº•"""
    try:
        # åªç¿»è¯‘æ‹‰ä¸åéƒ¨åˆ†ï¼Œå»æ‰ä½œè€…
        clean_text = text.split('(')[0].strip()
        return translator.translate(clean_text)
    except:
        return text

def fetch_baidu_page(url):
    """é€šç”¨è¯·æ±‚å‡½æ•°"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10, verify=False, allow_redirects=True)
        res.encoding = 'utf-8'
        return res
    except:
        return None

def parse_item_page(html):
    """è§£æå…·ä½“çš„è¯æ¡é¡µé¢"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. æŠ“æ ‡é¢˜ (ä¸­æ–‡å)
    zh_name = None
    h1 = soup.find('h1')
    if h1:
        zh_name = h1.get_text().strip()
    
    # å¦‚æœæ ‡é¢˜æ˜¯æ‹‰ä¸æ–‡ï¼Œå°è¯•åœ¨ Infobox é‡Œæ‰¾ "ä¸­æ–‡å"
    basic_infos = soup.find_all('dt', class_='basicInfo-item')
    for dt in basic_infos:
        if "ä¸­æ–‡å" in dt.get_text():
            next_dd = dt.find_next_sibling('dd')
            if next_dd:
                zh_name = next_dd.get_text().strip()
                break
    
    # 2. æŠ“æè¿° (æ‘˜è¦)
    desc = "æš‚æ— è¯¦ç»†æè¿°ã€‚"
    summary = soup.find('div', class_='lemma-summary') or soup.find('div', class_='J-summary')
    if summary:
        desc = summary.get_text().strip().replace("\n", "")
        desc = re.sub(r'\[.*?\]', '', desc) # å»æ‰ [1][2]
    
    return zh_name, desc

def get_plant_info(latin_name):
    clean_name = " ".join(latin_name.split()[:2]).strip()
    
    # --- ç¬¬ä¸€æ­¥ï¼šå‘èµ·æœç´¢ ---
    search_url = f"https://baike.baidu.com/search/word?word={clean_name}"
    response = fetch_baidu_page(search_url)
    
    if not response:
        return None

    final_html = response.text
    final_url = response.url

    # --- ç¬¬äºŒæ­¥ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦â€œäºŒçº§è·³è½¬â€ ---
    # å¦‚æœ URL åŒ…å« /search/ï¼Œè¯´æ˜æ²¡æœ‰ç›´æ¥è·³åˆ°è¯æ¡ï¼Œè€Œæ˜¯å±•ç¤ºäº†æœç´¢ç»“æœåˆ—è¡¨
    if "/search/" in final_url:
        soup = BeautifulSoup(final_html, 'html.parser')
        
        # å¯»æ‰¾æœç´¢ç»“æœçš„ç¬¬ä¸€æ¡é“¾æ¥
        # ç™¾åº¦æœç´¢ç»“æœé€šå¸¸åœ¨ a.result-title æˆ– h3 > a
        # è¿™é‡Œå°è¯•æŠ“å–ç¬¬ä¸€ä¸ªç»“æœ
        first_result = soup.find('a', class_='result-title')
        
        if first_result and first_result.get('href'):
            # æ‰¾åˆ°äº†ï¼æ¯”å¦‚ "æ¯’è´è‹£" çš„é“¾æ¥
            target_link = first_result['href']
            if not target_link.startswith('http'):
                target_link = "https://baike.baidu.com" + target_link
            
            # å†æ¬¡è¯·æ±‚è¿™ä¸ªå…·ä½“çš„è¯æ¡é¡µ
            # print(f" -> è¿½è¸ªè·³è½¬: {target_link}")
            sub_res = fetch_baidu_page(target_link)
            if sub_res:
                final_html = sub_res.text
            else:
                return None
        else:
            # æœç´¢ç»“æœé¡µéƒ½æ²¡æœ‰ä¸œè¥¿ï¼Œé‚£å°±æ˜¯çœŸæ²¡æœ‰äº†
            return None

    # --- ç¬¬ä¸‰æ­¥ï¼šè§£æé¡µé¢ ---
    zh_name, desc = parse_item_page(final_html)
    
    # æ ¡éªŒï¼šå¦‚æœåå­—æ²¡å–åˆ°ï¼Œæˆ–è€…æè¿°æ˜¯ç©ºçš„
    if not zh_name: 
        return None
        
    return {
        "zh_name": zh_name,
        "desc": desc,
        "habit": "è¯¦æƒ…è¯·è§æè¿°ã€‚"
    }

def run_crawler():
    if not os.path.exists(SOURCE_JSON):
        print("âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶")
        return

    with open(SOURCE_JSON, 'r', encoding='utf-8') as f:
        source_data = json.load(f)
        
    # è¯»å–ç°æœ‰è¿›åº¦
    final_dict = {}
    if os.path.exists(TARGET_JSON):
        with open(TARGET_JSON, 'r', encoding='utf-8') as f:
            try: 
                final_dict = json.load(f) 
            except: 
                pass

    print(f"ğŸ“‚ å½“å‰åº“ä¸­æœ‰ {len(final_dict)} æ¡ã€‚")

    # --- æ¸…æ´—æ— æ•ˆæ•°æ® ---
    # è¿™æ¬¡æˆ‘ä»¬ç‹ ä¸€ç‚¹ï¼Œåªè¦åå­—åŒ…å«æ‹‰ä¸æ–‡(å³æ²¡ç¿»è¯‘æˆåŠŸ)æˆ–è€…æè¿°å«ç³Šçš„ï¼Œå…¨éƒ¨é‡æŠ“
    keys_to_fix = []
    for k, v in final_dict.items():
        # è§„åˆ™ï¼šå¦‚æœä¸­æ–‡åå«æœ‰ Latin (æ¯”å¦‚ 'Lactuca') ä¸”æè¿°æ˜¯ 'æœ¬åœ°æ•°æ®åº“æš‚æœª...'ï¼Œåˆ™è§†ä¸ºå¤±è´¥ï¼Œåˆ æ‰é‡æ¥
        if "æœ¬åœ°æ•°æ®åº“æš‚æœª" in v['desc']:
            keys_to_fix.append(k)
    
    for k in keys_to_fix:
        del final_dict[k]
        
    if keys_to_fix:
        print(f"â™»ï¸  è‡ªåŠ¨åˆ é™¤äº† {len(keys_to_fix)} æ¡ä¹‹å‰çš„åƒåœ¾æ•°æ®ï¼Œå‡†å¤‡é‡æ–°è·å–...")

    # ä»»åŠ¡åˆ—è¡¨
    all_latin_names = list(source_data.values())
    todo_list = [name for name in all_latin_names if " ".join(name.split()[:2]).strip() not in final_dict]
    
    print(f"ğŸš€ å¼€å§‹æš´åŠ›æŠ“å– {len(todo_list)} ä¸ªè¯æ¡...")
    
    counter = 0
    for latin_name in tqdm(todo_list):
        clean_name = " ".join(latin_name.split()[:2]).strip()
        
        # 1. çˆ¬ç™¾åº¦
        info = get_plant_info(latin_name)
        
        if info:
            final_dict[clean_name] = info
        else:
            # 2. ç™¾åº¦å½»åº•å¤±è´¥ -> å¯ç”¨è°·æ­Œç¿»è¯‘
            # print(f" -> ç™¾åº¦æ— ç»“æœï¼Œè°ƒç”¨ç¿»è¯‘: {clean_name}")
            trans_name = safe_translate(clean_name)
            final_dict[clean_name] = {
                "zh_name": trans_name, # è‡³å°‘è¿™é‡Œæ˜¯ä¸­æ–‡ï¼
                "desc": "æš‚æ— è¯¦ç»†ç™¾ç§‘èµ„æ–™ï¼ˆå·²è‡ªåŠ¨ç¿»è¯‘åç§°ï¼‰ã€‚",
                "habit": "æœªçŸ¥"
            }
        
        counter += 1
        if counter % 5 == 0:
            with open(TARGET_JSON, 'w', encoding='utf-8') as f:
                json.dump(final_dict, f, ensure_ascii=False, indent=4)
        
        time.sleep(random.uniform(0.5, 1.2))

    with open(TARGET_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_dict, f, ensure_ascii=False, indent=4)
    print("\nâœ… æ‰€æœ‰æ•°æ®æŠ“å–/ä¿®å¤å®Œæˆï¼")

if __name__ == "__main__":
    run_crawler()